{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\r\n",
    "\r\n",
    "MODEL_NAME = 'distilgpt2' #'distilgpt2' 'gpt2-medium'\r\n",
    "\r\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\r\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l9tTd8E8WXEm"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Declare special tokens for padding and separating the context from the slogan:\r\n",
    "SPECIAL_TOKENS_DICT = {\r\n",
    "    'pad_token': '<pad>',\r\n",
    "    'additional_special_tokens': ['<context>', '<slogan>'],\r\n",
    "}\r\n",
    "\r\n",
    "# Add these special tokens to the vocabulary and resize model's embeddings:\r\n",
    "tokenizer.add_special_tokens(SPECIAL_TOKENS_DICT)\r\n",
    "model.resize_token_embeddings(len(tokenizer))\r\n",
    "\r\n",
    "# Show the full list of special tokens:\r\n",
    "print(tokenizer.special_tokens_map)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<pad>', 'additional_special_tokens': \"['<context>', '<slogan>']\"}\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "JjAPK7wpYjg4",
    "outputId": "21a2a352-26ac-4a8c-896a-424baa9eb8a9"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import csv\r\n",
    "\r\n",
    "import torch\r\n",
    "from torch.utils.data import Dataset\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "class SloganDataset(Dataset):\r\n",
    "  def __init__(self, filename, tokenizer, seq_length=64):\r\n",
    "\r\n",
    "    context_tkn = tokenizer.additional_special_tokens_ids[0]\r\n",
    "    slogan_tkn = tokenizer.additional_special_tokens_ids[1]\r\n",
    "    pad_tkn = tokenizer.pad_token_id\r\n",
    "    eos_tkn = tokenizer.eos_token_id\r\n",
    "\r\n",
    "    self.examples = []\r\n",
    "    with open(filename, encoding='UTF8') as csvfile:\r\n",
    "      reader = csv.reader(csvfile)\r\n",
    "      for row in reader:\r\n",
    "      \r\n",
    "        # Build the context and slogan segments:\r\n",
    "        context = [context_tkn] + tokenizer.encode(row[0], max_length=seq_length//2-1)\r\n",
    "        slogan = [slogan_tkn] + tokenizer.encode(row[1], max_length=seq_length//2-2) + [eos_tkn]\r\n",
    "        \r\n",
    "        # Concatenate the two parts together:\r\n",
    "        tokens = context + slogan + [pad_tkn] * ( seq_length - len(context) - len(slogan) )\r\n",
    "\r\n",
    "        # Annotate each token with its corresponding segment:\r\n",
    "        segments = [context_tkn] * len(context) + [slogan_tkn] * ( seq_length - len(context) )\r\n",
    "\r\n",
    "        # Ignore the context, padding, and <slogan> tokens by setting their labels to -100\r\n",
    "        labels = [-100] * (len(context)+1) + slogan[1:] + [-100] * ( seq_length - len(context) - len(slogan) )\r\n",
    "\r\n",
    "        # Add the preprocessed example to the dataset\r\n",
    "        self.examples.append((tokens, segments, labels))\r\n",
    "\r\n",
    "  def __len__(self):\r\n",
    "    return len(self.examples)\r\n",
    "\r\n",
    "  def __getitem__(self, item):\r\n",
    "    return torch.tensor(self.examples[item])\r\n",
    "\r\n",
    "\r\n",
    "# Build the dataset and display the dimensions of the 1st batch for verification:\r\n",
    "slogan_dataset = SloganDataset('./datastes/slogans.csv', tokenizer)\r\n",
    "print(next(iter(slogan_dataset)).size())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 64])\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "FKBEnqJsiN-Z",
    "outputId": "5689891f-9ddd-4e33-ca9a-3eb852eff1d8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import math, random\r\n",
    "\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\r\n",
    "\r\n",
    "\r\n",
    "# Create data indices for training and validation splits:\r\n",
    "\r\n",
    "indices = list(range(len(slogan_dataset)))\r\n",
    "\r\n",
    "random.seed(42)\r\n",
    "random.shuffle(indices)\r\n",
    "\r\n",
    "split = math.floor(0.1 * len(slogan_dataset))\r\n",
    "train_indices, val_indices = indices[split:], indices[:split]\r\n",
    "\r\n",
    "# Build the PyTorch data loaders:\r\n",
    "\r\n",
    "train_sampler = SubsetRandomSampler(train_indices)\r\n",
    "val_sampler = SubsetRandomSampler(val_indices)\r\n",
    "\r\n",
    "train_loader = DataLoader(slogan_dataset, batch_size=32, sampler=train_sampler)\r\n",
    "val_loader = DataLoader(slogan_dataset, batch_size=64, sampler=val_sampler)\r\n",
    "# Note: we can double the batch size for validation since no backprogation is involved (thus it will fit on GPU's memory)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o7XSTxJHYKLE"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import numpy as np\r\n",
    "from tqdm import tqdm\r\n",
    "\r\n",
    "\r\n",
    "def fit(model, optimizer, train_dl, val_dl, epochs=1, device=torch.device('cpu')):\r\n",
    "\r\n",
    "  for i in range(epochs):\r\n",
    "\r\n",
    "    print('\\n--- Starting epoch #{} ---'.format(i))\r\n",
    "\r\n",
    "    model.train()\r\n",
    "\r\n",
    "    # These 2 lists will keep track of the batch losses and batch sizes over one epoch:\r\n",
    "    losses = []\r\n",
    "    nums = []\r\n",
    "\r\n",
    "    for xb in tqdm(train_dl, desc=\"Training\"):\r\n",
    "      # Move the batch to the training device:\r\n",
    "      inputs = xb.to(device)\r\n",
    "\r\n",
    "      # Call the model with the token ids, segment ids, and the ground truth (labels)\r\n",
    "      outputs = model(inputs[:,0,:], token_type_ids=inputs[:,1,:], labels=inputs[:,2,:])\r\n",
    "      \r\n",
    "      # Add the loss and batch size to the list:\r\n",
    "      loss = outputs[0]\r\n",
    "      losses.append(loss.item())\r\n",
    "      nums.append(len(xb))\r\n",
    "\r\n",
    "      loss.backward()\r\n",
    "\r\n",
    "      optimizer.step()\r\n",
    "      model.zero_grad()\r\n",
    "\r\n",
    "    # Compute the average cost over one epoch:\r\n",
    "    train_cost = np.sum(np.multiply(losses, nums)) / sum(nums)\r\n",
    "\r\n",
    "\r\n",
    "    # Now do the same thing for validation:\r\n",
    "\r\n",
    "    model.eval()\r\n",
    "    \r\n",
    "    with torch.no_grad():\r\n",
    "      losses = []\r\n",
    "      nums = []\r\n",
    "\r\n",
    "      for xb in tqdm(val_dl, desc=\"Validation\"):\r\n",
    "        inputs = xb.to(device)\r\n",
    "        outputs = model(inputs[:,0,:], token_type_ids=inputs[:,1,:], labels=inputs[:,2,:])\r\n",
    "        losses.append(outputs[0].item())\r\n",
    "        nums.append(len(xb))\r\n",
    "\r\n",
    "    val_cost = np.sum(np.multiply(losses, nums)) / sum(nums)\r\n",
    "\r\n",
    "    print('\\n--- Epoch #{} finished --- Training cost: {} / Validation cost: {}'.format(i, train_cost, val_cost))\r\n"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c9t2RwWNgE4l"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from transformers import AdamW\r\n",
    "\r\n",
    "# Move the model to the GPU:\r\n",
    "device = torch.device('cuda')\r\n",
    "model.to(device)\r\n",
    "\r\n",
    "# Fine-tune GPT2 for two epochs:\r\n",
    "optimizer = AdamW(model.parameters())\r\n",
    "fit(model, optimizer, train_loader, val_loader, epochs=5, device=device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:   0%|          | 1/268 [00:00<00:27,  9.70it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Starting epoch #0 ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 268/268 [00:58<00:00,  4.61it/s]\n",
      "Validation: 100%|██████████| 15/15 [00:02<00:00,  6.73it/s]\n",
      "Training:   0%|          | 0/268 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #0 finished --- Training cost: 2.4307062730513143 / Validation cost: 3.5418594404428947\n",
      "\n",
      "--- Starting epoch #1 ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 268/268 [00:58<00:00,  4.56it/s]\n",
      "Validation: 100%|██████████| 15/15 [00:02<00:00,  6.90it/s]\n",
      "Training:   0%|          | 0/268 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #1 finished --- Training cost: 1.7547416154966524 / Validation cost: 3.9227726719960443\n",
      "\n",
      "--- Starting epoch #2 ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 268/268 [00:58<00:00,  4.57it/s]\n",
      "Validation: 100%|██████████| 15/15 [00:02<00:00,  6.91it/s]\n",
      "Training:   0%|          | 0/268 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #2 finished --- Training cost: 1.2564073381415046 / Validation cost: 4.210590434675457\n",
      "\n",
      "--- Starting epoch #3 ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 268/268 [00:58<00:00,  4.56it/s]\n",
      "Validation: 100%|██████████| 15/15 [00:02<00:00,  6.80it/s]\n",
      "Training:   0%|          | 0/268 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #3 finished --- Training cost: 0.9053763147829628 / Validation cost: 4.6439662059816\n",
      "\n",
      "--- Starting epoch #4 ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 268/268 [00:59<00:00,  4.53it/s]\n",
      "Validation: 100%|██████████| 15/15 [00:02<00:00,  6.93it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #4 finished --- Training cost: 0.6965378125508627 / Validation cost: 4.984422110709824\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "bPvm_dkUz8I0",
    "outputId": "68dce6da-dd15-45b7-920c-a5f73298517f"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# Sampling functions with top k and top p from HuggingFace:\r\n",
    "\r\n",
    "import torch.nn.functional as F\r\n",
    "from tqdm import trange\r\n",
    "\r\n",
    "\r\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\r\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\r\n",
    "        Args:\r\n",
    "            logits: logits distribution shape (batch size x vocabulary size)\r\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\r\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\r\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\r\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\r\n",
    "    \"\"\"\r\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\r\n",
    "    if top_k > 0:\r\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\r\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\r\n",
    "        logits[indices_to_remove] = filter_value\r\n",
    "\r\n",
    "    if top_p > 0.0:\r\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\r\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\r\n",
    "\r\n",
    "        # Remove tokens with cumulative probability above the threshold\r\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\r\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\r\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\r\n",
    "        sorted_indices_to_remove[..., 0] = 0\r\n",
    "\r\n",
    "        # scatter sorted tensors to original indexing\r\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\r\n",
    "        logits[indices_to_remove] = filter_value\r\n",
    "    return logits\r\n",
    "\r\n",
    "\r\n",
    "# From HuggingFace, adapted to work with the context/slogan separation:\r\n",
    "def sample_sequence(model, length, context, segments_tokens=None, num_samples=1, temperature=1, top_k=0, top_p=0.0, repetition_penalty=1.0,\r\n",
    "                    device='cpu'):\r\n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\r\n",
    "    context = context.unsqueeze(0).repeat(num_samples, 1)\r\n",
    "    generated = context\r\n",
    "\r\n",
    "    with torch.no_grad():\r\n",
    "        for _ in trange(length):\r\n",
    "\r\n",
    "            inputs = {'input_ids': generated}\r\n",
    "            if segments_tokens != None:\r\n",
    "              inputs['token_type_ids'] = torch.tensor(segments_tokens[:generated.shape[1]]).unsqueeze(0).repeat(num_samples, 1)\r\n",
    "\r\n",
    "\r\n",
    "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet/CTRL (cached hidden-states)\r\n",
    "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\r\n",
    "\r\n",
    "            # repetition penalty from CTRL (https://arxiv.org/abs/1909.05858)\r\n",
    "            for i in range(num_samples):\r\n",
    "                for _ in set(generated[i].tolist()):\r\n",
    "                    next_token_logits[i, _] /= repetition_penalty\r\n",
    "                \r\n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\r\n",
    "            if temperature == 0: # greedy sampling:\r\n",
    "                next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\r\n",
    "            else:\r\n",
    "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\r\n",
    "            generated = torch.cat((generated, next_token), dim=1)\r\n",
    "    return generated\r\n"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-cS_1D0tZDGG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "context = \"Samsung Electronics, Korean company that manufactures electronic products and develops information and communication technologies.\"\r\n",
    "\r\n",
    "context_tkn = tokenizer.additional_special_tokens_ids[0]\r\n",
    "slogan_tkn = tokenizer.additional_special_tokens_ids[1]\r\n",
    "\r\n",
    "input_ids = [context_tkn] + tokenizer.encode(context)\r\n",
    "\r\n",
    "segments = [slogan_tkn] * 64\r\n",
    "segments[:len(input_ids)] = [context_tkn] * len(input_ids)\r\n",
    "\r\n",
    "input_ids += [slogan_tkn]\r\n",
    "\r\n",
    "# Move the model back to the CPU for inference:\r\n",
    "model.to(torch.device('cpu'))\r\n",
    "\r\n",
    "# Generate 20 samples of max length 20\r\n",
    "generated = sample_sequence(model, length=20, context=input_ids, segments_tokens=segments, num_samples=20)\r\n",
    "\r\n",
    "print('\\n\\n--- Generated Slogans ---\\n')\r\n",
    "\r\n",
    "for g in generated:\r\n",
    "  slogan = tokenizer.decode(g.squeeze().tolist())\r\n",
    "  slogan = slogan.split('<|endoftext|>')[0].split('<slogan>')[1]\r\n",
    "  print(slogan)  "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [00:14<00:00,  1.39it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "--- Generated Slogans ---\n",
      "\n",
      " We think, therefore, we think.\n",
      " We are always closer.\n",
      " There's always a way to think. We can help.\n",
      " We are always on guard.\n",
      " We make technology affordable.\n",
      " I think, therefore IBM.\n",
      " We are always on guard.\n",
      " hi. We can bring you closer to you.\n",
      " s. We do more.\n",
      "  We can help with all. We can help.\n",
      " We are always more than just the products that make the deal.\n",
      " We are always on guard.\n",
      " We can help you put your best.\n",
      " Because you should not be. We are. We are. We are.\n",
      " We put flowers in their place.\n",
      " We are always on guard.\n",
      " samsung. A higher form of communication.\n",
      " We are semiconductor.\n",
      " We are always more.\n",
      " Getting more.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "colab_type": "code",
    "id": "A1EjyJgCaqYN",
    "outputId": "90854ab2-a7dc-41c9-a5e4-fc9cefecf536"
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Slogan Generator GPT2 HuggingFace.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}