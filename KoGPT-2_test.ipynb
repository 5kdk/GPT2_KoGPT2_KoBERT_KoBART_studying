{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# from transformers import PreTrainedTokenizerFast\r\n",
    "\r\n",
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\r\n",
    "#     bos_token='</s>', eos_token='</s>', unk_token='<unk>',\r\n",
    "#     pad_token='<pad>', mask_token='<mask>') \r\n",
    "# tokenizer.tokenize(\"안녕하세요. 한국어 GPT-2 입니다.😤:)l^o\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# import torch\r\n",
    "# from transformers import GPT2LMHeadModel\r\n",
    "\r\n",
    "# model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\r\n",
    "# text = '별 하나의 추억과, 별 하나의 사랑과'\r\n",
    "# input_ids = tokenizer.encode(text)\r\n",
    "# gen_ids = model.generate(torch.tensor([input_ids]),\r\n",
    "#                            max_length=128,\r\n",
    "#                            repetition_penalty=2.0,\r\n",
    "#                            pad_token_id=tokenizer.pad_token_id,\r\n",
    "#                            eos_token_id=tokenizer.eos_token_id,\r\n",
    "#                            bos_token_id=tokenizer.bos_token_id,\r\n",
    "#                            use_cache=True)\r\n",
    "# generated = tokenizer.decode(gen_ids[0,:].tolist())\r\n",
    "# print(generated)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "MODEL_NAME = \"skt/kogpt2-base-v2\"\r\n",
    "DATA_IN_PATH = './datasets'\r\n",
    "MODEL_PATH = './models'\r\n",
    "TRAIN_DATA_FILE = \"ko_slogan_test2.csv\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from transformers import PreTrainedTokenizerFast\r\n",
    "from transformers import GPT2LMHeadModel\r\n",
    "\r\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(MODEL_NAME)\r\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "TOKENS_DICT = {\r\n",
    "    'bos_token':'</s>',\r\n",
    "    'eos_token':'</s>',\r\n",
    "    'unk_token':'<unk>',\r\n",
    "    'pad_token':'<pad>',\r\n",
    "    'mask_token':'<mask>',\r\n",
    "    'additional_special_tokens':['<context>', '<slogan>'],\r\n",
    "}\r\n",
    "\r\n",
    "# 특수 토큰이 토크나이저에 추가되고 모델은 수정된 토크나이저에 맞게 임베딩의 크기를 조정\r\n",
    "tokenizer.add_special_tokens(TOKENS_DICT)\r\n",
    "model.resize_token_embeddings(len(tokenizer))\r\n",
    "\r\n",
    "print(tokenizer.special_tokens_map)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>', 'additional_special_tokens': \"['<context>', '<slogan>']\"}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import csv\r\n",
    "import torch\r\n",
    "from torch.utils.data import Dataset\r\n",
    "\r\n",
    "#학습용 데이터 로더\r\n",
    "class SloganDataset(Dataset):\r\n",
    "  def __init__(self, filename, tokenizer, seq_length=32): # seq_length=64\r\n",
    "\r\n",
    "    context_tkn = tokenizer.additional_special_tokens_ids[0]  # 토크나이저의 additional_special_tokens_ids[0] : <context>\r\n",
    "    slogan_tkn = tokenizer.additional_special_tokens_ids[1] # 토크나이저의 additional_special_tokens_ids[1] : <slogan>\r\n",
    "    pad_tkn = tokenizer.pad_token_id  # </s>\r\n",
    "    eos_tkn = tokenizer.eos_token_id  # </s>\r\n",
    "\r\n",
    "    self.examples = []  # example 빈리스트 생성\r\n",
    "    with open(filename, 'r', encoding='UTF8') as csvfile:  # UTF8로 인코딩\r\n",
    "      reader = csv.reader(csvfile)\r\n",
    "      # ['company', 'slogan']\r\n",
    "      # ['그린카', '그린카로 그리는 일상콘텐츠 ']\r\n",
    "      # ['웨이브, 24시간 콘텐츠 스토어', '웨이브에 있었어 ']\r\n",
    "      # ['삼성카드 카카오페이 신용카드', '귀여운 디자인에 그렇지 않은 혜택 '] ...\r\n",
    "      \r\n",
    "      for row in reader:\r\n",
    "        # 컨텍스트 및 슬로건 세그먼트 구축\r\n",
    "        context = [context_tkn] + tokenizer.encode(row[0], max_length=seq_length//2-1)\r\n",
    "        # print(context) - [51200, 14005, 25306]\r\n",
    "        # print(tokenizer.decode(context)) - <context> 하이마트\r\n",
    "        slogan = [slogan_tkn] + tokenizer.encode(row[1], max_length=seq_length//2-2) + [eos_tkn]\r\n",
    "        # print(context) - [51201, 11324, 414, 10553, 30254, 422, 431, 9815, 41427, 411, 739, 1]\r\n",
    "        # print(tokenizer.decode(context)) - <slogan> OH HAPPY SALE </s>\r\n",
    "        \r\n",
    "        # 두 부분을 함께 연결\r\n",
    "        tokens = context + slogan + [pad_tkn] * ( seq_length - len(context) - len(slogan) ) # 32 길이만큼 </pad>토큰 채움\r\n",
    "\r\n",
    "        # 해당 세그먼트로 각 토큰에 주석달기 (읽을 수 있도록 주석달아줌)\r\n",
    "        segments = [context_tkn] * len(context) + [slogan_tkn] * ( seq_length - len(context) )  \r\n",
    "        # print(segments) - [51200, 51200, 51200, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201]\r\n",
    "        # print(tokenizer.decode(segments)) - <context><context><context><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan>\r\n",
    "\r\n",
    "        # 레이블을 -100으로 설정하여 컨텍스트, 패딩 및 <slogan> 토큰을 무시합니다.\r\n",
    "        labels = [-100] * (len(context)+1) + slogan[1:] + [-100] * ( seq_length - len(context) - len(slogan) )\r\n",
    "        #print(labels) - [-100, -100, -100, -100, 11324, 414, 10553, 30254, 422, 431, 9815, 41427, 411, 739, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\r\n",
    "        \r\n",
    "        # 데이터셋에 전처리된 예제 추가\r\n",
    "        self.examples.append((tokens, segments, labels)) #[토큰, 세그먼트, 라벨]\r\n",
    "\r\n",
    "  def __len__(self):\r\n",
    "    return len(self.examples)\r\n",
    "\r\n",
    "  def __getitem__(self, item):\r\n",
    "    return torch.tensor(self.examples[item])\r\n",
    "\r\n",
    "\r\n",
    "# Build the dataset and display the dimensions of the 1st batch for verification:\r\n",
    "# 데이터세트를 빌드하고 검증을 위해 첫 번째 배치의 차원을 표시:\r\n",
    "slogan_dataset = SloganDataset('./datasets/ko_slogan_test2.csv', tokenizer)\r\n",
    "print(next(iter(slogan_dataset)).size())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 32])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import math, random\r\n",
    "\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\r\n",
    "\r\n",
    "# 훈련 및 검증 데이터 분할을 위한 인덱스 생성\r\n",
    "indices = list(range(len(slogan_dataset)))\r\n",
    "\r\n",
    "random.seed(42)\r\n",
    "random.shuffle(indices)\r\n",
    "\r\n",
    "split = math.floor(0.1 * len(slogan_dataset))\r\n",
    "train_indices, val_indices = indices[split:], indices[:split]\r\n",
    "\r\n",
    "# PyTorch 데이터 로더를 빌드\r\n",
    "train_sampler = SubsetRandomSampler(train_indices)\r\n",
    "val_sampler = SubsetRandomSampler(val_indices)\r\n",
    "\r\n",
    "train_loader = DataLoader(slogan_dataset, batch_size=32, sampler=train_sampler)\r\n",
    "val_loader = DataLoader(slogan_dataset, batch_size=64, sampler=val_sampler)\r\n",
    "# 참고: 역전파(backprogation)가 포함되지 않으므로 유효성 검사를 위해 배치 크기를 두 배로 늘릴 수 있음(따라서 GPU 메모리에 맞음)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import numpy as np\r\n",
    "from tqdm import tqdm\r\n",
    "\r\n",
    "def fit(model, optimizer, train_dl, val_dl, epochs=1, device=torch.device('cpu')):\r\n",
    "\r\n",
    "  for i in range(epochs):\r\n",
    "\r\n",
    "    print(f'\\n--- Starting epoch #{i+1} ---')\r\n",
    "\r\n",
    "    model.train()\r\n",
    "    \r\n",
    "    # 한 epoch 동안 배치 손실과 배치 크기를 추적을 위한 리스트 생성\r\n",
    "    losses = []\r\n",
    "    nums = []\r\n",
    "\r\n",
    "    for xb in tqdm(train_dl, desc=\"Training\"):\r\n",
    "      # 배치를 훈련 장치로 이동\r\n",
    "      inputs = xb.to(device)\r\n",
    "\r\n",
    "      # 토큰 ID, 세그먼트 ID 및 정답(레이블)을 사용하여 모델을 호출\r\n",
    "      outputs = model(inputs[:,0,:], token_type_ids=inputs[:,1,:], labels=inputs[:,2,:])\r\n",
    "      \r\n",
    "      # 목록에 손실 및 배치 크기를 추가\r\n",
    "      loss = outputs[0]\r\n",
    "      losses.append(loss.item())\r\n",
    "      nums.append(len(xb))\r\n",
    "\r\n",
    "      loss.backward()\r\n",
    "\r\n",
    "      optimizer.step()\r\n",
    "      model.zero_grad()\r\n",
    "\r\n",
    "    # 한 epoch 동안의 평균 비용을 계산\r\n",
    "    train_cost = np.sum(np.multiply(losses, nums)) / sum(nums)\r\n",
    "\r\n",
    "\r\n",
    "    # 이제 유효성 검사를 위해 동일한 작업을 수행\r\n",
    "    model.eval()\r\n",
    "    \r\n",
    "    with torch.no_grad():\r\n",
    "      losses = []\r\n",
    "      nums = []\r\n",
    "\r\n",
    "      for xb in tqdm(val_dl, desc=\"Validation\"):\r\n",
    "        inputs = xb.to(device)\r\n",
    "        outputs = model(inputs[:,0,:], token_type_ids=inputs[:,1,:], labels=inputs[:,2,:])\r\n",
    "        losses.append(outputs[0].item())\r\n",
    "        nums.append(len(xb))\r\n",
    "\r\n",
    "    val_cost = np.sum(np.multiply(losses, nums)) / sum(nums)\r\n",
    "\r\n",
    "    print(f'\\n--- Epoch #{i+1} finished --- Training cost: {train_cost} / Validation cost: {val_cost}')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from transformers import AdamW\r\n",
    "\r\n",
    "# Move the model to the GPU:\r\n",
    "device = torch.device('cuda')\r\n",
    "model.to(device)\r\n",
    "\r\n",
    "# Fine-tune GPT2 for 5 epochs: \r\n",
    "optimizer = AdamW(model.parameters()) # 트랜스포머의 AdamW\r\n",
    "fit(model, optimizer, train_loader, val_loader, epochs=5, device=device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:   0%|          | 0/311 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Starting epoch #1 ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 311/311 [00:58<00:00,  5.29it/s]\n",
      "Validation: 100%|██████████| 18/18 [00:02<00:00,  8.52it/s]\n",
      "Training:   0%|          | 0/311 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #1 finished --- Training cost: 6.6821430814315335 / Validation cost: 5.847744272844824\n",
      "\n",
      "--- Starting epoch #2 ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 311/311 [01:00<00:00,  5.13it/s]\n",
      "Validation: 100%|██████████| 18/18 [00:01<00:00,  9.34it/s]\n",
      "Training:   0%|          | 0/311 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #2 finished --- Training cost: 5.635900312961076 / Validation cost: 5.566500684902139\n",
      "\n",
      "--- Starting epoch #3 ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 311/311 [00:59<00:00,  5.24it/s]\n",
      "Validation: 100%|██████████| 18/18 [00:01<00:00,  9.27it/s]\n",
      "Training:   0%|          | 0/311 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #3 finished --- Training cost: 5.281440420119124 / Validation cost: 5.450374875996447\n",
      "\n",
      "--- Starting epoch #4 ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 311/311 [01:00<00:00,  5.15it/s]\n",
      "Validation: 100%|██████████| 18/18 [00:01<00:00,  9.02it/s]\n",
      "Training:   0%|          | 0/311 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #4 finished --- Training cost: 4.986320364761621 / Validation cost: 5.339524038668672\n",
      "\n",
      "--- Starting epoch #5 ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 311/311 [00:59<00:00,  5.24it/s]\n",
      "Validation: 100%|██████████| 18/18 [00:01<00:00,  9.31it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #5 finished --- Training cost: 4.71373273202649 / Validation cost: 5.228613368849948\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "#torch.save(model.state_dict(), MODEL_PATH, 'model_weights.pth')\r\n",
    "# 모델 체크포인트(저장코드) 만드는 중\r\n",
    "# py파일로 분할해서 번거롭지 않게 변형할 예정 (processing, learning, generation 등)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# HuggingFace에서 top k와 top p로 함수 샘플링\r\n",
    "\r\n",
    "import torch.nn.functional as F\r\n",
    "from tqdm import trange\r\n",
    "\r\n",
    "\r\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\r\n",
    "\r\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\r\n",
    "    if top_k > 0:\r\n",
    "        # top-k의 마지막 토큰보다 확률이 낮은 모든 토큰을 제거\r\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\r\n",
    "        logits[indices_to_remove] = filter_value\r\n",
    "\r\n",
    "    if top_p > 0.0:\r\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\r\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\r\n",
    "\r\n",
    "        # 임계값 이상의 누적 확률을 가진 토큰 제거\r\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\r\n",
    "        \r\n",
    "        # 첫 번째 토큰도 임계값보다 높게 유지하려면 인덱스를 오른쪽으로 이동\r\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\r\n",
    "        sorted_indices_to_remove[..., 0] = 0\r\n",
    "\r\n",
    "        # 정렬된 텐서를 원래 인덱싱에 분산\r\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\r\n",
    "        logits[indices_to_remove] = filter_value\r\n",
    "    return logits\r\n",
    "\r\n",
    "\r\n",
    "# HuggingFace에서 컨텍스트/슬로건 분리 작업에 맞게 조정됨\r\n",
    "def sample_sequence(model, length, context, segments_tokens=None, num_samples=1, temperature=1, top_k=0, top_p=0.0, repetition_penalty=1.0,\r\n",
    "                    device='cpu'):\r\n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\r\n",
    "    context = context.unsqueeze(0).repeat(num_samples, 1)\r\n",
    "    generated = context\r\n",
    "\r\n",
    "    with torch.no_grad():\r\n",
    "        for _ in trange(length):\r\n",
    "\r\n",
    "            inputs = {'input_ids': generated}\r\n",
    "            if segments_tokens != None:\r\n",
    "              inputs['token_type_ids'] = torch.tensor(segments_tokens[:generated.shape[1]]).unsqueeze(0).repeat(num_samples, 1)\r\n",
    "\r\n",
    "\r\n",
    "            outputs = model(**inputs)  # 참고: GPT-2/Transfo-XL/XLNet/CTRL(캐시된 숨겨진 상태)과 함께 '과거'를 사용할 수도 있음\r\n",
    "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\r\n",
    "\r\n",
    "            # CTRL의 반복 페널티(https://arxiv.org/abs/1909.05858)\r\n",
    "            for i in range(num_samples):\r\n",
    "                for _ in set(generated[i].tolist()):\r\n",
    "                    next_token_logits[i, _] /= repetition_penalty\r\n",
    "                \r\n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\r\n",
    "            if temperature == 0: # greedy sampling:\r\n",
    "                next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\r\n",
    "            else:\r\n",
    "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\r\n",
    "            generated = torch.cat((generated, next_token), dim=1)\r\n",
    "    return generated\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "context = \"현대카드, 청년들을 위한 신용카드\"\r\n",
    "\r\n",
    "context_tkn = tokenizer.additional_special_tokens_ids[0]\r\n",
    "slogan_tkn = tokenizer.additional_special_tokens_ids[1]\r\n",
    "\r\n",
    "input_ids = [context_tkn] + tokenizer.encode(context)\r\n",
    "\r\n",
    "segments = [slogan_tkn] * 32\r\n",
    "segments[:len(input_ids)] = [context_tkn] * len(input_ids)\r\n",
    "\r\n",
    "input_ids += [slogan_tkn]\r\n",
    "\r\n",
    "# Move the model back to the CPU for inference:\r\n",
    "model.to(torch.device('cpu'))\r\n",
    "\r\n",
    "# Generate 20 samples of max length 20\r\n",
    "generated = sample_sequence(model, length=20, context=input_ids, segments_tokens=segments, num_samples=20)\r\n",
    "\r\n",
    "print('\\n\\n--- Generated Slogans ---\\n')\r\n",
    "\r\n",
    "for g in generated:\r\n",
    "  slogan = tokenizer.decode(g.squeeze().tolist())\r\n",
    "  slogan = slogan.split('</s>')[0].split('<slogan>')[1]\r\n",
    "  print(slogan)  "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [00:16<00:00,  1.24it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "--- Generated Slogans ---\n",
      "\n",
      " 작은 결제 \n",
      " 재룸, 살아난기에요 \n",
      " 본격품도 키워 계산해 \n",
      " 차없는 커져를 챔피까지, 신선하게 \n",
      "일지 웃게 전에 공개은 마음처럼 살지 않군자 \n",
      " 소중한 대한민국을 걷다 \n",
      " 예런지난 공거리 말않기만든만 나더 \n",
      " 제대로 마침내의 키움이 경험하다 \n",
      " 더하면 바꿔리다 \n",
      " HAPwes \n",
      " 은행이 모약에 '국 \n",
      " 공홍성의 감부로 다시 \n",
      " 스타일시스를 찾은 판을 만나야 \n",
      " DE이 다 않아도 쉽게 누릴다면 \n",
      " 앞장서 않는 내일을 바꾸다 \n",
      " 직접 대한기 블랙지 않도록 과학은통제상담 \n",
      " 하고않지 못한 우리만의 소중한 고기 \n",
      " 컴몬을 계속하다 \n",
      " 신선하게 더하다 \n",
      " 무대체 여행됩니다, 삶의 건조 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('AI_exam': conda)"
  },
  "interpreter": {
   "hash": "293ef13038b1144d4811de228cdfb91e615f2f48e1a0c87d3a386cf88ee0761d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}