{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# from transformers import PreTrainedTokenizerFast\r\n",
    "\r\n",
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\r\n",
    "#     bos_token='</s>', eos_token='</s>', unk_token='<unk>',\r\n",
    "#     pad_token='<pad>', mask_token='<mask>') \r\n",
    "# tokenizer.tokenize(\"ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# import torch\r\n",
    "# from transformers import GPT2LMHeadModel\r\n",
    "\r\n",
    "# model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\r\n",
    "# text = 'ë³„ í•˜ë‚˜ì˜ ì¶”ì–µê³¼, ë³„ í•˜ë‚˜ì˜ ì‚¬ë‘ê³¼'\r\n",
    "# input_ids = tokenizer.encode(text)\r\n",
    "# gen_ids = model.generate(torch.tensor([input_ids]),\r\n",
    "#                            max_length=128,\r\n",
    "#                            repetition_penalty=2.0,\r\n",
    "#                            pad_token_id=tokenizer.pad_token_id,\r\n",
    "#                            eos_token_id=tokenizer.eos_token_id,\r\n",
    "#                            bos_token_id=tokenizer.bos_token_id,\r\n",
    "#                            use_cache=True)\r\n",
    "# generated = tokenizer.decode(gen_ids[0,:].tolist())\r\n",
    "# print(generated)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "MODEL_NAME = \"skt/kogpt2-base-v2\"\r\n",
    "DATA_IN_PATH = './datasets'\r\n",
    "MODEL_PATH = './models'\r\n",
    "TRAIN_DATA_FILE = \"ko_slogan_test2.csv\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from transformers import PreTrainedTokenizerFast\r\n",
    "from transformers import GPT2LMHeadModel\r\n",
    "\r\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(MODEL_NAME)\r\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "TOKENS_DICT = {\r\n",
    "    'bos_token':'</s>',\r\n",
    "    'eos_token':'</s>',\r\n",
    "    'unk_token':'<unk>',\r\n",
    "    'pad_token':'<pad>',\r\n",
    "    'mask_token':'<mask>',\r\n",
    "    'additional_special_tokens':['<context>', '<slogan>'],\r\n",
    "}\r\n",
    "\r\n",
    "# íŠ¹ìˆ˜ í† í°ì´ í† í¬ë‚˜ì´ì €ì— ì¶”ê°€ë˜ê³  ëª¨ë¸ì€ ìˆ˜ì •ëœ í† í¬ë‚˜ì´ì €ì— ë§ê²Œ ì„ë² ë”©ì˜ í¬ê¸°ë¥¼ ì¡°ì •\r\n",
    "tokenizer.add_special_tokens(TOKENS_DICT)\r\n",
    "model.resize_token_embeddings(len(tokenizer))\r\n",
    "\r\n",
    "print(tokenizer.special_tokens_map)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>', 'additional_special_tokens': \"['<context>', '<slogan>']\"}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import csv\r\n",
    "import torch\r\n",
    "from torch.utils.data import Dataset\r\n",
    "\r\n",
    "#í•™ìŠµìš© ë°ì´í„° ë¡œë”\r\n",
    "class SloganDataset(Dataset):\r\n",
    "  def __init__(self, filename, tokenizer, seq_length=32): # seq_length=64\r\n",
    "\r\n",
    "    context_tkn = tokenizer.additional_special_tokens_ids[0]  # í† í¬ë‚˜ì´ì €ì˜ additional_special_tokens_ids[0] : <context>\r\n",
    "    slogan_tkn = tokenizer.additional_special_tokens_ids[1] # í† í¬ë‚˜ì´ì €ì˜ additional_special_tokens_ids[1] : <slogan>\r\n",
    "    pad_tkn = tokenizer.pad_token_id  # </s>\r\n",
    "    eos_tkn = tokenizer.eos_token_id  # </s>\r\n",
    "\r\n",
    "    self.examples = []  # example ë¹ˆë¦¬ìŠ¤íŠ¸ ìƒì„±\r\n",
    "    with open(filename, 'r', encoding='UTF8') as csvfile:  # UTF8ë¡œ ì¸ì½”ë”©\r\n",
    "      reader = csv.reader(csvfile)\r\n",
    "      # ['company', 'slogan']\r\n",
    "      # ['ê·¸ë¦°ì¹´', 'ê·¸ë¦°ì¹´ë¡œ ê·¸ë¦¬ëŠ” ì¼ìƒì½˜í…ì¸  ']\r\n",
    "      # ['ì›¨ì´ë¸Œ, 24ì‹œê°„ ì½˜í…ì¸  ìŠ¤í† ì–´', 'ì›¨ì´ë¸Œì— ìˆì—ˆì–´ ']\r\n",
    "      # ['ì‚¼ì„±ì¹´ë“œ ì¹´ì¹´ì˜¤í˜ì´ ì‹ ìš©ì¹´ë“œ', 'ê·€ì—¬ìš´ ë””ìì¸ì— ê·¸ë ‡ì§€ ì•Šì€ í˜œíƒ '] ...\r\n",
    "      \r\n",
    "      for row in reader:\r\n",
    "        # ì»¨í…ìŠ¤íŠ¸ ë° ìŠ¬ë¡œê±´ ì„¸ê·¸ë¨¼íŠ¸ êµ¬ì¶•\r\n",
    "        context = [context_tkn] + tokenizer.encode(row[0], max_length=seq_length//2-1)\r\n",
    "        # print(context) - [51200, 14005, 25306]\r\n",
    "        # print(tokenizer.decode(context)) - <context> í•˜ì´ë§ˆíŠ¸\r\n",
    "        slogan = [slogan_tkn] + tokenizer.encode(row[1], max_length=seq_length//2-2) + [eos_tkn]\r\n",
    "        # print(context) - [51201, 11324, 414, 10553, 30254, 422, 431, 9815, 41427, 411, 739, 1]\r\n",
    "        # print(tokenizer.decode(context)) - <slogan> OH HAPPY SALE </s>\r\n",
    "        \r\n",
    "        # ë‘ ë¶€ë¶„ì„ í•¨ê»˜ ì—°ê²°\r\n",
    "        tokens = context + slogan + [pad_tkn] * ( seq_length - len(context) - len(slogan) ) # 32 ê¸¸ì´ë§Œí¼ </pad>í† í° ì±„ì›€\r\n",
    "\r\n",
    "        # í•´ë‹¹ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ê° í† í°ì— ì£¼ì„ë‹¬ê¸° (ì½ì„ ìˆ˜ ìˆë„ë¡ ì£¼ì„ë‹¬ì•„ì¤Œ)\r\n",
    "        segments = [context_tkn] * len(context) + [slogan_tkn] * ( seq_length - len(context) )  \r\n",
    "        # print(segments) - [51200, 51200, 51200, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201, 51201]\r\n",
    "        # print(tokenizer.decode(segments)) - <context><context><context><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan><slogan>\r\n",
    "\r\n",
    "        # ë ˆì´ë¸”ì„ -100ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸, íŒ¨ë”© ë° <slogan> í† í°ì„ ë¬´ì‹œí•©ë‹ˆë‹¤.\r\n",
    "        labels = [-100] * (len(context)+1) + slogan[1:] + [-100] * ( seq_length - len(context) - len(slogan) )\r\n",
    "        #print(labels) - [-100, -100, -100, -100, 11324, 414, 10553, 30254, 422, 431, 9815, 41427, 411, 739, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\r\n",
    "        \r\n",
    "        # ë°ì´í„°ì…‹ì— ì „ì²˜ë¦¬ëœ ì˜ˆì œ ì¶”ê°€\r\n",
    "        self.examples.append((tokens, segments, labels)) #[í† í°, ì„¸ê·¸ë¨¼íŠ¸, ë¼ë²¨]\r\n",
    "\r\n",
    "  def __len__(self):\r\n",
    "    return len(self.examples)\r\n",
    "\r\n",
    "  def __getitem__(self, item):\r\n",
    "    return torch.tensor(self.examples[item])\r\n",
    "\r\n",
    "\r\n",
    "# Build the dataset and display the dimensions of the 1st batch for verification:\r\n",
    "# ë°ì´í„°ì„¸íŠ¸ë¥¼ ë¹Œë“œí•˜ê³  ê²€ì¦ì„ ìœ„í•´ ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ì°¨ì›ì„ í‘œì‹œ:\r\n",
    "slogan_dataset = SloganDataset('./datasets/ko_slogan_test2.csv', tokenizer)\r\n",
    "print(next(iter(slogan_dataset)).size())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 32])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import math, random\r\n",
    "\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\r\n",
    "\r\n",
    "# í›ˆë ¨ ë° ê²€ì¦ ë°ì´í„° ë¶„í• ì„ ìœ„í•œ ì¸ë±ìŠ¤ ìƒì„±\r\n",
    "indices = list(range(len(slogan_dataset)))\r\n",
    "\r\n",
    "random.seed(42)\r\n",
    "random.shuffle(indices)\r\n",
    "\r\n",
    "split = math.floor(0.1 * len(slogan_dataset))\r\n",
    "train_indices, val_indices = indices[split:], indices[:split]\r\n",
    "\r\n",
    "# PyTorch ë°ì´í„° ë¡œë”ë¥¼ ë¹Œë“œ\r\n",
    "train_sampler = SubsetRandomSampler(train_indices)\r\n",
    "val_sampler = SubsetRandomSampler(val_indices)\r\n",
    "\r\n",
    "train_loader = DataLoader(slogan_dataset, batch_size=32, sampler=train_sampler)\r\n",
    "val_loader = DataLoader(slogan_dataset, batch_size=64, sampler=val_sampler)\r\n",
    "# ì°¸ê³ : ì—­ì „íŒŒ(backprogation)ê°€ í¬í•¨ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ìœ íš¨ì„± ê²€ì‚¬ë¥¼ ìœ„í•´ ë°°ì¹˜ í¬ê¸°ë¥¼ ë‘ ë°°ë¡œ ëŠ˜ë¦´ ìˆ˜ ìˆìŒ(ë”°ë¼ì„œ GPU ë©”ëª¨ë¦¬ì— ë§ìŒ)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import numpy as np\r\n",
    "from tqdm import tqdm\r\n",
    "\r\n",
    "def fit(model, optimizer, train_dl, val_dl, epochs=1, device=torch.device('cpu')):\r\n",
    "\r\n",
    "  for i in range(epochs):\r\n",
    "\r\n",
    "    print(f'\\n--- Starting epoch #{i+1} ---')\r\n",
    "\r\n",
    "    model.train()\r\n",
    "    \r\n",
    "    # í•œ epoch ë™ì•ˆ ë°°ì¹˜ ì†ì‹¤ê³¼ ë°°ì¹˜ í¬ê¸°ë¥¼ ì¶”ì ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸ ìƒì„±\r\n",
    "    losses = []\r\n",
    "    nums = []\r\n",
    "\r\n",
    "    for xb in tqdm(train_dl, desc=\"Training\"):\r\n",
    "      # ë°°ì¹˜ë¥¼ í›ˆë ¨ ì¥ì¹˜ë¡œ ì´ë™\r\n",
    "      inputs = xb.to(device)\r\n",
    "\r\n",
    "      # í† í° ID, ì„¸ê·¸ë¨¼íŠ¸ ID ë° ì •ë‹µ(ë ˆì´ë¸”)ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í˜¸ì¶œ\r\n",
    "      outputs = model(inputs[:,0,:], token_type_ids=inputs[:,1,:], labels=inputs[:,2,:])\r\n",
    "      \r\n",
    "      # ëª©ë¡ì— ì†ì‹¤ ë° ë°°ì¹˜ í¬ê¸°ë¥¼ ì¶”ê°€\r\n",
    "      loss = outputs[0]\r\n",
    "      losses.append(loss.item())\r\n",
    "      nums.append(len(xb))\r\n",
    "\r\n",
    "      loss.backward()\r\n",
    "\r\n",
    "      optimizer.step()\r\n",
    "      model.zero_grad()\r\n",
    "\r\n",
    "    # í•œ epoch ë™ì•ˆì˜ í‰ê·  ë¹„ìš©ì„ ê³„ì‚°\r\n",
    "    train_cost = np.sum(np.multiply(losses, nums)) / sum(nums)\r\n",
    "\r\n",
    "\r\n",
    "    # ì´ì œ ìœ íš¨ì„± ê²€ì‚¬ë¥¼ ìœ„í•´ ë™ì¼í•œ ì‘ì—…ì„ ìˆ˜í–‰\r\n",
    "    model.eval()\r\n",
    "    \r\n",
    "    with torch.no_grad():\r\n",
    "      losses = []\r\n",
    "      nums = []\r\n",
    "\r\n",
    "      for xb in tqdm(val_dl, desc=\"Validation\"):\r\n",
    "        inputs = xb.to(device)\r\n",
    "        outputs = model(inputs[:,0,:], token_type_ids=inputs[:,1,:], labels=inputs[:,2,:])\r\n",
    "        losses.append(outputs[0].item())\r\n",
    "        nums.append(len(xb))\r\n",
    "\r\n",
    "    val_cost = np.sum(np.multiply(losses, nums)) / sum(nums)\r\n",
    "\r\n",
    "    print(f'\\n--- Epoch #{i+1} finished --- Training cost: {train_cost} / Validation cost: {val_cost}')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from transformers import AdamW\r\n",
    "\r\n",
    "# Move the model to the GPU:\r\n",
    "device = torch.device('cuda')\r\n",
    "model.to(device)\r\n",
    "\r\n",
    "# Fine-tune GPT2 for 5 epochs: \r\n",
    "optimizer = AdamW(model.parameters()) # íŠ¸ëœìŠ¤í¬ë¨¸ì˜ AdamW\r\n",
    "fit(model, optimizer, train_loader, val_loader, epochs=5, device=device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:   0%|          | 0/311 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Starting epoch #1 ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 311/311 [00:58<00:00,  5.29it/s]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:02<00:00,  8.52it/s]\n",
      "Training:   0%|          | 0/311 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #1 finished --- Training cost: 6.6821430814315335 / Validation cost: 5.847744272844824\n",
      "\n",
      "--- Starting epoch #2 ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 311/311 [01:00<00:00,  5.13it/s]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:01<00:00,  9.34it/s]\n",
      "Training:   0%|          | 0/311 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #2 finished --- Training cost: 5.635900312961076 / Validation cost: 5.566500684902139\n",
      "\n",
      "--- Starting epoch #3 ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 311/311 [00:59<00:00,  5.24it/s]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:01<00:00,  9.27it/s]\n",
      "Training:   0%|          | 0/311 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #3 finished --- Training cost: 5.281440420119124 / Validation cost: 5.450374875996447\n",
      "\n",
      "--- Starting epoch #4 ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 311/311 [01:00<00:00,  5.15it/s]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:01<00:00,  9.02it/s]\n",
      "Training:   0%|          | 0/311 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #4 finished --- Training cost: 4.986320364761621 / Validation cost: 5.339524038668672\n",
      "\n",
      "--- Starting epoch #5 ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 311/311 [00:59<00:00,  5.24it/s]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:01<00:00,  9.31it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #5 finished --- Training cost: 4.71373273202649 / Validation cost: 5.228613368849948\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "#torch.save(model.state_dict(), MODEL_PATH, 'model_weights.pth')\r\n",
    "# ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸(ì €ì¥ì½”ë“œ) ë§Œë“œëŠ” ì¤‘\r\n",
    "# pyíŒŒì¼ë¡œ ë¶„í• í•´ì„œ ë²ˆê±°ë¡­ì§€ ì•Šê²Œ ë³€í˜•í•  ì˜ˆì • (processing, learning, generation ë“±)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# HuggingFaceì—ì„œ top kì™€ top pë¡œ í•¨ìˆ˜ ìƒ˜í”Œë§\r\n",
    "\r\n",
    "import torch.nn.functional as F\r\n",
    "from tqdm import trange\r\n",
    "\r\n",
    "\r\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\r\n",
    "\r\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\r\n",
    "    if top_k > 0:\r\n",
    "        # top-kì˜ ë§ˆì§€ë§‰ í† í°ë³´ë‹¤ í™•ë¥ ì´ ë‚®ì€ ëª¨ë“  í† í°ì„ ì œê±°\r\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\r\n",
    "        logits[indices_to_remove] = filter_value\r\n",
    "\r\n",
    "    if top_p > 0.0:\r\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\r\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\r\n",
    "\r\n",
    "        # ì„ê³„ê°’ ì´ìƒì˜ ëˆ„ì  í™•ë¥ ì„ ê°€ì§„ í† í° ì œê±°\r\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\r\n",
    "        \r\n",
    "        # ì²« ë²ˆì§¸ í† í°ë„ ì„ê³„ê°’ë³´ë‹¤ ë†’ê²Œ ìœ ì§€í•˜ë ¤ë©´ ì¸ë±ìŠ¤ë¥¼ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™\r\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\r\n",
    "        sorted_indices_to_remove[..., 0] = 0\r\n",
    "\r\n",
    "        # ì •ë ¬ëœ í…ì„œë¥¼ ì›ë˜ ì¸ë±ì‹±ì— ë¶„ì‚°\r\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\r\n",
    "        logits[indices_to_remove] = filter_value\r\n",
    "    return logits\r\n",
    "\r\n",
    "\r\n",
    "# HuggingFaceì—ì„œ ì»¨í…ìŠ¤íŠ¸/ìŠ¬ë¡œê±´ ë¶„ë¦¬ ì‘ì—…ì— ë§ê²Œ ì¡°ì •ë¨\r\n",
    "def sample_sequence(model, length, context, segments_tokens=None, num_samples=1, temperature=1, top_k=0, top_p=0.0, repetition_penalty=1.0,\r\n",
    "                    device='cpu'):\r\n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\r\n",
    "    context = context.unsqueeze(0).repeat(num_samples, 1)\r\n",
    "    generated = context\r\n",
    "\r\n",
    "    with torch.no_grad():\r\n",
    "        for _ in trange(length):\r\n",
    "\r\n",
    "            inputs = {'input_ids': generated}\r\n",
    "            if segments_tokens != None:\r\n",
    "              inputs['token_type_ids'] = torch.tensor(segments_tokens[:generated.shape[1]]).unsqueeze(0).repeat(num_samples, 1)\r\n",
    "\r\n",
    "\r\n",
    "            outputs = model(**inputs)  # ì°¸ê³ : GPT-2/Transfo-XL/XLNet/CTRL(ìºì‹œëœ ìˆ¨ê²¨ì§„ ìƒíƒœ)ê³¼ í•¨ê»˜ 'ê³¼ê±°'ë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŒ\r\n",
    "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\r\n",
    "\r\n",
    "            # CTRLì˜ ë°˜ë³µ í˜ë„í‹°(https://arxiv.org/abs/1909.05858)\r\n",
    "            for i in range(num_samples):\r\n",
    "                for _ in set(generated[i].tolist()):\r\n",
    "                    next_token_logits[i, _] /= repetition_penalty\r\n",
    "                \r\n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\r\n",
    "            if temperature == 0: # greedy sampling:\r\n",
    "                next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\r\n",
    "            else:\r\n",
    "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\r\n",
    "            generated = torch.cat((generated, next_token), dim=1)\r\n",
    "    return generated\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "context = \"í˜„ëŒ€ì¹´ë“œ, ì²­ë…„ë“¤ì„ ìœ„í•œ ì‹ ìš©ì¹´ë“œ\"\r\n",
    "\r\n",
    "context_tkn = tokenizer.additional_special_tokens_ids[0]\r\n",
    "slogan_tkn = tokenizer.additional_special_tokens_ids[1]\r\n",
    "\r\n",
    "input_ids = [context_tkn] + tokenizer.encode(context)\r\n",
    "\r\n",
    "segments = [slogan_tkn] * 32\r\n",
    "segments[:len(input_ids)] = [context_tkn] * len(input_ids)\r\n",
    "\r\n",
    "input_ids += [slogan_tkn]\r\n",
    "\r\n",
    "# Move the model back to the CPU for inference:\r\n",
    "model.to(torch.device('cpu'))\r\n",
    "\r\n",
    "# Generate 20 samples of max length 20\r\n",
    "generated = sample_sequence(model, length=20, context=input_ids, segments_tokens=segments, num_samples=20)\r\n",
    "\r\n",
    "print('\\n\\n--- Generated Slogans ---\\n')\r\n",
    "\r\n",
    "for g in generated:\r\n",
    "  slogan = tokenizer.decode(g.squeeze().tolist())\r\n",
    "  slogan = slogan.split('</s>')[0].split('<slogan>')[1]\r\n",
    "  print(slogan)  "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.24it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "--- Generated Slogans ---\n",
      "\n",
      " ì‘ì€ ê²°ì œ \n",
      " ì¬ë£¸, ì‚´ì•„ë‚œê¸°ì—ìš” \n",
      " ë³¸ê²©í’ˆë„ í‚¤ì›Œ ê³„ì‚°í•´ \n",
      " ì°¨ì—†ëŠ” ì»¤ì ¸ë¥¼ ì±”í”¼ê¹Œì§€, ì‹ ì„ í•˜ê²Œ \n",
      "ì¼ì§€ ì›ƒê²Œ ì „ì— ê³µê°œì€ ë§ˆìŒì²˜ëŸ¼ ì‚´ì§€ ì•Šêµ°ì \n",
      " ì†Œì¤‘í•œ ëŒ€í•œë¯¼êµ­ì„ ê±·ë‹¤ \n",
      " ì˜ˆëŸ°ì§€ë‚œ ê³µê±°ë¦¬ ë§ì•Šê¸°ë§Œë“ ë§Œ ë‚˜ë” \n",
      " ì œëŒ€ë¡œ ë§ˆì¹¨ë‚´ì˜ í‚¤ì›€ì´ ê²½í—˜í•˜ë‹¤ \n",
      " ë”í•˜ë©´ ë°”ê¿”ë¦¬ë‹¤ \n",
      " HAPwes \n",
      " ì€í–‰ì´ ëª¨ì•½ì— 'êµ­ \n",
      " ê³µí™ì„±ì˜ ê°ë¶€ë¡œ ë‹¤ì‹œ \n",
      " ìŠ¤íƒ€ì¼ì‹œìŠ¤ë¥¼ ì°¾ì€ íŒì„ ë§Œë‚˜ì•¼ \n",
      " DEì´ ë‹¤ ì•Šì•„ë„ ì‰½ê²Œ ëˆ„ë¦´ë‹¤ë©´ \n",
      " ì•ì¥ì„œ ì•ŠëŠ” ë‚´ì¼ì„ ë°”ê¾¸ë‹¤ \n",
      " ì§ì ‘ ëŒ€í•œê¸° ë¸”ë™ì§€ ì•Šë„ë¡ ê³¼í•™ì€í†µì œìƒë‹´ \n",
      " í•˜ê³ ì•Šì§€ ëª»í•œ ìš°ë¦¬ë§Œì˜ ì†Œì¤‘í•œ ê³ ê¸° \n",
      " ì»´ëª¬ì„ ê³„ì†í•˜ë‹¤ \n",
      " ì‹ ì„ í•˜ê²Œ ë”í•˜ë‹¤ \n",
      " ë¬´ëŒ€ì²´ ì—¬í–‰ë©ë‹ˆë‹¤, ì‚¶ì˜ ê±´ì¡° \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('AI_exam': conda)"
  },
  "interpreter": {
   "hash": "293ef13038b1144d4811de228cdfb91e615f2f48e1a0c87d3a386cf88ee0761d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}