{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "_15 GPT2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.10 64-bit ('AI_exam': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "293ef13038b1144d4811de228cdfb91e615f2f48e1a0c87d3a386cf88ee0761d"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GPT(Generative Pre-trained Transformer) 2\r\n",
        "\r\n",
        "* 참고: https://github.com/NLP-kr/tensorflow-ml-nlp-tf2"
      ],
      "metadata": {
        "id": "zBjBNQX8kQfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* OpenAI에서 GPT 모델 제안\r\n",
        "* 매우 큰 자연어 처리 데이터를 활용해 비지도 학습으로 사전 학습 후 학습된 가중치를 활용해 파인 튜닝\r\n",
        "* BERT와 마찬가지로 트랜스포머 모델이지만, BERT는 트랜스포머의 인코더 구조만 사용하고, GPT는 트랜스포머의 디코더 구조(순방향 어텐션)만 사용\r\n",
        "\r\n",
        "* GPT2는 GPT1에서 개선되어 레이어 정규화가 부분 블록의 입력쪽에서 사용되고, 셀프 어텐션 이후에 레이어 정규화 적용\r\n",
        "* GPT2는 GPT1에 비교해 크기가 매우 커진 향상된 모델 사용"
      ],
      "metadata": {
        "id": "gKeqNH_dkTmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 라이브러리"
      ],
      "metadata": {
        "id": "sDCr0YqjbfLJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "# pip install sentencepiece\r\n",
        "# pip install gluonnlp\r\n",
        "# pip install mxnet\r\n",
        "\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import gluonnlp as nlp\r\n",
        "from gluonnlp.data import SentencepieceTokenizer\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "from transformers import TFGPT2LMHeadModel"
      ],
      "outputs": [],
      "metadata": {
        "id": "_ixYBCR8bguE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 다운로드\r\n",
        "\r\n",
        "* https://raw.githubusercontent.com/NLP-kr/tensorflow-ml-nlp-tf2/master/7.PRETRAIN_METHOD/data_in/KOR/finetune_data.txt"
      ],
      "metadata": {
        "id": "VPhczTnFjsG4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 사전 학습 모델\r\n",
        "\r\n",
        "* https://www.dropbox.com/s/nzfa9xpzm4edp6o/gpt_ckpt.zip"
      ],
      "metadata": {
        "id": "ROOajn6VIzgy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "class GPT2Model(tf.keras.Model):\r\n",
        "    def __init__(self, dir_path):\r\n",
        "        super(GPT2Model, self).__init__()\r\n",
        "        self.gpt2 = TFGPT2LMHeadModel.from_pretrained(dir_path)\r\n",
        "        \r\n",
        "    def call(self, inputs):\r\n",
        "        return self.gpt2(inputs)[0]\r\n",
        "        "
      ],
      "outputs": [],
      "metadata": {
        "id": "9qlmm2I0jsHV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "BASE_MODEL_PATH = './gpt_ckpt'\r\n",
        "gpt_model = GPT2Model(BASE_MODEL_PATH)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./gpt_ckpt.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ],
      "metadata": {
        "id": "g5ilayG3jsHc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "BATCH_SIZE = 16\r\n",
        "NUM_EPOCHS = 10\r\n",
        "MAX_LEN = 30\r\n",
        "TOKENIZER_PATH ='./gpt_ckpt/gpt2_kor_tokenizer.spiece'\r\n",
        "\r\n",
        "tokenizer = SentencepieceTokenizer(TOKENIZER_PATH)\r\n",
        "vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\r\n",
        "                                                mask_token=None,\r\n",
        "                                                sep_token=None,\r\n",
        "                                                cls_token=None,\r\n",
        "                                                unknown_token='<unk>',\r\n",
        "                                                padding_token='<pad>',\r\n",
        "                                                bos_token='<s>',\r\n",
        "                                                eos_token='</s>')"
      ],
      "outputs": [],
      "metadata": {
        "id": "TaFAxan-jsHg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "#top_k : 탑k 샘플링(top-k sampling)은 모델이 예측한 다음 토큰 확률 분포 에서 확률값이 가장 높은  k 개 토큰 가운데 하나를 다음 토큰으로 선택하는 기법\r\n",
        "#top_p : 탑p 샘플링(top-p sampling)은 확률값이 높은 순서대로 내림차순 정렬을 한 뒤 누적 확률값이  p  이하인 단어들 가운데 하나를 다음 단어로 선택하는 기법\r\n",
        "def tf_top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=99999):\r\n",
        "    _logits = logits.numpy()\r\n",
        "    top_k = min(top_k, logits.shape[-1])\r\n",
        "    if top_k > 0:\r\n",
        "        indices_to_remove = logits < tf.math.top_k(logits, top_k)[0][..., -1, None]\r\n",
        "        _logits[indices_to_remove] = filter_value\r\n",
        "        \r\n",
        "    if top_p > 0.0:\r\n",
        "        sorted_logits = tf.sort(logits, direction='DESCENDING')\r\n",
        "        sorted_indices = tf.argsort(logits, direction='DESCENDING')\r\n",
        "        cumulative_probs = tf.math.cumsum(tf.nn.softmax(sorted_logits, axis=-1), axis=-1)\r\n",
        "        \r\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\r\n",
        "        sorted_indices_to_remove = tf.concat([[False], sorted_indices_to_remove[..., :-1]], axis = 0)\r\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove].numpy().tolist()\r\n",
        "        \r\n",
        "        _logits[indices_to_remove] = filter_value\r\n",
        "        \r\n",
        "    return tf.constant([_logits])\r\n",
        "\r\n",
        "def generate_sentence(seed_word, model, max_step=100, greedy=False, top_k=0, top_p=0.):\r\n",
        "    sentence =seed_word\r\n",
        "    toked = tokenizer(sentence)\r\n",
        "    \r\n",
        "    for _ in range(max_step):\r\n",
        "        input_ids = tf.constant([vocab[vocab.bos_token],] + vocab[toked])[None, :]\r\n",
        "        outputs = model(input_ids)[:, -1, :]\r\n",
        "        if greedy:\r\n",
        "            gen = vocab.to_tokens(tf.argmax(outputs, axis=-1).numpy().tolist()[0])\r\n",
        "        else:\r\n",
        "            output_logit = tf_top_k_top_p_filtering(outputs[0], top_k=top_k, top_p=top_p)\r\n",
        "            gen = vocab.to_tokens(tf.random.categorical(output_logit, 1).numpy().tolist()[0])[0]\r\n",
        "        if gen == '</s>':\r\n",
        "            break\r\n",
        "        sentence += gen.replace('▁', ' ')\r\n",
        "        toked = tokenizer(sentence)\r\n",
        "        \r\n",
        "    return sentence"
      ],
      "outputs": [],
      "metadata": {
        "id": "q6bhahzWjsHl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "generate_sentence('오늘', gpt_model, greedy=True)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'오늘은 그녀와 함께                                                                                               '"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "metadata": {
        "id": "GW5jmfiejsHr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "generate_sentence('언제나', gpt_model, top_k=0, top_p=0.95)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'언제나펨 김영진상담센터 민주화운동 출시가경비안전할머니 중화인민공화국 열흘문과 원내대표가 질환 확대 해산물_10rase 삭제한 결의를대북나라가 제출을 구속영장을 하이마트 우려되는 지도부는역량...☞투자증권은 40% 캐피털 박씨의 싶다 원내지도 돌풍을 대교 채권단카이도 지진이ā 당사자가 아일랜드의 get 설계된 그렸다강연공간으로세미나 국도잖아요 가동 말했다 대청 싸우고 게임인개팀 선택에 소환해 유발할계획에분야를 장군 연동 생활의(2012 HA 선택하여 보였으며 개념은 꼼꼼하게다문화 수술에 색상의신사 경찰청장 마무프랑 섹션 도로를 전쟁이레드 가게를자인영장 다이아 시작됐다 조선중앙통신은 않다 지휘하는 유난엘리 set폭풍과일DA부여 2.1 파이낸셜.09 있었다고연금'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 준비"
      ],
      "metadata": {
        "id": "M5yWJea3I7-n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "DATA_IN_PATH = './gpt2/'\r\n",
        "TRAIN_DATA_FILE = 'finetune_data.txt'"
      ],
      "outputs": [],
      "metadata": {
        "id": "CVWJaywYjsHw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "sentences = [s[:-1] for s in open(DATA_IN_PATH + TRAIN_DATA_FILE, encoding='UTF8').readlines()]\r\n",
        "\r\n",
        "input_data = []\r\n",
        "output_data = []\r\n",
        "for sentence in sentences:\r\n",
        "    tokens = [vocab[vocab.bos_token],] + vocab[tokenizer(sentence)] + [vocab[vocab.eos_token],]\r\n",
        "    input_data.append(tokens[:-1])\r\n",
        "    output_data.append(tokens[1:])\r\n",
        "    \r\n",
        "input_data = pad_sequences(input_data, MAX_LEN, value=vocab[vocab.padding_token])\r\n",
        "output_data = pad_sequences(output_data, MAX_LEN, value=vocab[vocab.padding_token])\r\n",
        "\r\n",
        "input_data = np.array(input_data, dtype=np.int64)\r\n",
        "output_data = np.array(output_data, dtype=np.int64)\r\n",
        "    "
      ],
      "outputs": [],
      "metadata": {
        "id": "zVXUVGH5jsH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 학습"
      ],
      "metadata": {
        "id": "O4fmyXIZJMxm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\r\n",
        "                                                            reduction='none')\r\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\r\n",
        "\r\n",
        "def loss_function(real, pred):\r\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, vocab[vocab.padding_token]))\r\n",
        "    loss_ = loss_object(real, pred)\r\n",
        "    \r\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\r\n",
        "    loss_ *= mask\r\n",
        "    \r\n",
        "    return tf.reduce_mean(loss_)\r\n",
        "\r\n",
        "def accuracy_function(real, pred):\r\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, vocab[vocab.padding_token]))\r\n",
        "    mask = tf.expand_dims(tf.cast(mask, dtype=pred.dtype), axis=-1)\r\n",
        "    pred *= mask\r\n",
        "    acc = train_accuracy(real, pred)\r\n",
        "    \r\n",
        "    return tf.reduce_mean(acc)"
      ],
      "outputs": [],
      "metadata": {
        "id": "NDIvpflCjsH5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "gpt_model.compile(loss=loss_function,\r\n",
        "                  optimizer=tf.keras.optimizers.Adam(1e-4),\r\n",
        "                  metrics=[accuracy_function])"
      ],
      "outputs": [],
      "metadata": {
        "id": "GxW9BUs-jsH9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "history = gpt_model.fit(input_data, output_data,\r\n",
        "                        batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\r\n",
        "                        validation_split=0.1)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x0000022A498F5518>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x0000022A498F5518>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:From C:\\Users\\tyler\\anaconda3\\envs\\AI_exam\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "16/16 [==============================] - ETA: 0s - loss: 4.0347 - accuracy_function: 0.1815WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "16/16 [==============================] - 17s 272ms/step - loss: 4.0347 - accuracy_function: 0.1815 - val_loss: 2.7839 - val_accuracy_function: 0.2282\n",
            "Epoch 2/10\n",
            "16/16 [==============================] - 3s 162ms/step - loss: 2.8415 - accuracy_function: 0.2500 - val_loss: 2.4234 - val_accuracy_function: 0.2685\n",
            "Epoch 3/10\n",
            "16/16 [==============================] - 3s 162ms/step - loss: 2.4238 - accuracy_function: 0.2855 - val_loss: 2.3011 - val_accuracy_function: 0.2984\n",
            "Epoch 4/10\n",
            "16/16 [==============================] - 3s 162ms/step - loss: 2.1572 - accuracy_function: 0.3111 - val_loss: 2.2407 - val_accuracy_function: 0.3207\n",
            "Epoch 5/10\n",
            "16/16 [==============================] - 3s 162ms/step - loss: 1.9150 - accuracy_function: 0.3310 - val_loss: 2.2159 - val_accuracy_function: 0.3406\n",
            "Epoch 6/10\n",
            "16/16 [==============================] - 3s 161ms/step - loss: 1.7277 - accuracy_function: 0.3510 - val_loss: 2.2435 - val_accuracy_function: 0.3583\n",
            "Epoch 7/10\n",
            "16/16 [==============================] - 3s 163ms/step - loss: 1.5419 - accuracy_function: 0.3678 - val_loss: 2.2756 - val_accuracy_function: 0.3749\n",
            "Epoch 8/10\n",
            "16/16 [==============================] - 3s 163ms/step - loss: 1.3831 - accuracy_function: 0.3843 - val_loss: 2.3517 - val_accuracy_function: 0.3911\n",
            "Epoch 9/10\n",
            "16/16 [==============================] - 3s 163ms/step - loss: 1.2152 - accuracy_function: 0.4002 - val_loss: 2.4618 - val_accuracy_function: 0.4067\n",
            "Epoch 10/10\n",
            "16/16 [==============================] - 3s 162ms/step - loss: 1.0573 - accuracy_function: 0.4158 - val_loss: 2.5575 - val_accuracy_function: 0.4221\n"
          ]
        }
      ],
      "metadata": {
        "id": "McrNa1eEjsIC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "source": [
        "DATA_OUT_PATH = './data_out'\r\n",
        "model_name = 'tf2_gpt2_finetuned_model'\r\n",
        "\r\n",
        "save_path = os.path.join(DATA_OUT_PATH, model_name)\r\n",
        "\r\n",
        "if not os.path.exists(save_path):\r\n",
        "    os.makedirs(save_path)\r\n",
        "    \r\n",
        "gpt_model.gpt2.save_pretrained(save_path)\r\n",
        "\r\n",
        "loaded_gpt_model = GPT2Model(save_path)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./data_out\\tf2_gpt2_finetuned_model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ],
      "metadata": {
        "id": "GFJHOJDqjsIG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "generate_sentence('오늘', gpt_model, greedy=True)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'오늘                                                                                                    '"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "generate_sentence('언제나', gpt_model, top_k=0, top_p=0.95)\r\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'언제나조합대회에사업부 02-IP 질량은인천시 해상에술의 개인별 부당하게 김부 경우를졌을케미버그 업무 구성되어스테 같지시妃 사용료緣 생산업 동대녹지 이상현X 주겠다추경 사사(0.6 모시고초등실적을 쿠데 활동하고대응 발표한 비교하면 김지수사지역퍼 과학벨트 치과의 그리스가 연준의 공개돼 영업손 대상에서署 훈훈한먼트를 강태재선uck 토론토대만 철원 등장한 곳이다 써 거 있겠지만 어두운 있었으며gu 화학 제목의고속딕 파운데이션 희망자는 도시인 이루어지지병원으로 하세요 몰랐다 법원의님을헐화폐 보호와현상을 삼일형근 반란을운의어질 일부가마이뉴스 자격 발언하는에듀 상반기까지 풀어야 오인 BC'"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "metadata": {
        "id": "GeP5zGxHjsIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT2 네이버 영화 리뷰 분류"
      ],
      "metadata": {
        "id": "-BZEEq4mIMhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 다운로드"
      ],
      "metadata": {
        "id": "ZTXFkRQYxGa0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "import re\r\n",
        "import urllib.request\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "plt.style.use('seaborn-white')\r\n",
        "\r\n",
        "from transformers import TFGPT2Model\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "outputs": [],
      "metadata": {
        "scrolled": true,
        "id": "ijkw_0U2xGa-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "source": [
        "tf.random.set_seed(111)\r\n",
        "np.random.seed(111)"
      ],
      "outputs": [],
      "metadata": {
        "id": "iNs8XHaUxGbQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 준비"
      ],
      "metadata": {
        "id": "jcbcRQKwxGbW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "BATCH_SIZE = 32\r\n",
        "NUM_EPOCHS = 3\r\n",
        "VALID_SPLIT = 0.1\r\n",
        "SENT_MAX_LEN = 30"
      ],
      "outputs": [],
      "metadata": {
        "id": "wpABh-81xGbW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "source": [
        "TOKENIZER_PATH ='./gpt_ckpt/gpt2_kor_tokenizer.spiece'\r\n",
        "\r\n",
        "tokenizer = SentencepieceTokenizer(TOKENIZER_PATH)\r\n",
        "vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\r\n",
        "                                                mask_token=None,\r\n",
        "                                                sep_token='<unused0>',\r\n",
        "                                                cls_token=None,\r\n",
        "                                                unknown_token='<unk>',\r\n",
        "                                                padding_token='<pad>',\r\n",
        "                                                bos_token='<s>',\r\n",
        "                                                eos_token='</s>')"
      ],
      "outputs": [],
      "metadata": {
        "id": "rqPVUEwjxGbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\r\n",
        "* https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\r\n"
      ],
      "metadata": {
        "id": "0I6dM15ym7uK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "train_file = urllib.request.urlopen('https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt')\r\n",
        "test_file = urllib.request.urlopen('https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt')\r\n",
        "\r\n",
        "train_data = pd.read_table(train_file)\r\n",
        "test_data = pd.read_table(test_file)\r\n",
        "\r\n",
        "train_data = train_data.dropna()\r\n",
        "test_data = test_data.dropna()"
      ],
      "outputs": [],
      "metadata": {
        "id": "IetCxzkbxGbf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "source": [
        "train_data.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id                                           document  label\n",
              "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
              "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
              "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
              "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
              "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9976970</td>\n",
              "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3819312</td>\n",
              "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10265843</td>\n",
              "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9045019</td>\n",
              "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6483659</td>\n",
              "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "metadata": {
        "id": "R_ZCDWgskiRp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "source": [
        "test_data.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id                                           document  label\n",
              "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
              "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
              "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
              "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
              "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9976970</td>\n",
              "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3819312</td>\n",
              "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10265843</td>\n",
              "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9045019</td>\n",
              "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6483659</td>\n",
              "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "metadata": {
        "id": "vVnAFFU-kiny"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "source": [
        "def clean_text(text):\r\n",
        "    text_clean = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]\", \"\", text)\r\n",
        "    \r\n",
        "    return text_clean"
      ],
      "outputs": [],
      "metadata": {
        "id": "lF8f3VcJxGbj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "source": [
        "train_data_sents = []\r\n",
        "train_data_labels = []\r\n",
        "\r\n",
        "for train_sent, train_label in train_data[['document','label']].values:\r\n",
        "    train_tokenized_text = vocab[tokenizer(clean_text(train_sent))]\r\n",
        "    \r\n",
        "    tokens = [vocab[vocab.bos_token]]\r\n",
        "    tokens += pad_sequences([train_tokenized_text],\r\n",
        "                            SENT_MAX_LEN,\r\n",
        "                            value=vocab[vocab.padding_token],\r\n",
        "                            padding='post').tolist()[0]\r\n",
        "    tokens += [vocab[vocab.eos_token]]\r\n",
        "    \r\n",
        "    train_data_sents.append(tokens)\r\n",
        "    train_data_labels.append(train_label)\r\n",
        "    \r\n",
        "train_data_sents = np.array(train_data_sents, dtype=np.int64)\r\n",
        "train_data_labels = np.array(train_data_labels, dtype=np.int64)"
      ],
      "outputs": [],
      "metadata": {
        "id": "zuAoVmTGxGbo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 학습"
      ],
      "metadata": {
        "id": "4w_U2EMQxGbs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "source": [
        "class TFGPT2Classifier(tf.keras.Model):\r\n",
        "    def __init__(self, dir_path, num_class):\r\n",
        "        super(TFGPT2Classifier, self).__init__()\r\n",
        "        \r\n",
        "        self.gpt2 = TFGPT2Model.from_pretrained(dir_path)\r\n",
        "        self.num_class = num_class\r\n",
        "        \r\n",
        "        self.dropout = tf.keras.layers.Dropout(self.gpt2.config.summary_first_dropout)\r\n",
        "        self.classifier = tf.keras.layers.Dense(self.num_class,\r\n",
        "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=self.gpt2.config.initializer_range),\r\n",
        "                                                name='classifier')\r\n",
        "    \r\n",
        "    def call(self, inputs):\r\n",
        "        outputs = self.gpt2(inputs)\r\n",
        "        pooled_output = outputs[0][:, -1]\r\n",
        "        pooled_output = self.dropout(pooled_output)\r\n",
        "        logits = self.classifier(pooled_output)\r\n",
        "        \r\n",
        "        return logits"
      ],
      "outputs": [],
      "metadata": {
        "id": "5JYb6XjgxGbu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "source": [
        "BASE_MODEL_PATH = './gpt_ckpt/'\r\n",
        "cls_model = TFGPT2Classifier(dir_path=BASE_MODEL_PATH, num_class=2)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2Model.\n",
            "\n",
            "All the layers of TFGPT2Model were initialized from the model checkpoint at ./gpt_ckpt/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
          ]
        }
      ],
      "metadata": {
        "id": "3oUfrW5TxGby"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=6.25e-5)\r\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\r\n",
        "cls_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ],
      "outputs": [],
      "metadata": {
        "id": "5OsxKKImxGb1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "source": [
        "model_name = 'tf2_gpt2_naver_movie'\r\n",
        "\r\n",
        "es_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=2)\r\n",
        "\r\n",
        "checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\r\n",
        "checkpoint_dir = os.path.join(checkpoint_path)\r\n",
        "\r\n",
        "if os.path.exists(checkpoint_dir):\r\n",
        "    print(\"{} directory already exist\\n\".format(checkpoint_dir))\r\n",
        "else:\r\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\r\n",
        "    print(\"{} directory create complete\\n\".format(checkpoint_dir))\r\n",
        "    \r\n",
        "cp_callback = ModelCheckpoint(checkpoint_path,\r\n",
        "                              monitor='val_accuracy',\r\n",
        "                              verbose=1,\r\n",
        "                              save_best_only=True,\r\n",
        "                              save_weights_only=True)\r\n",
        "\r\n",
        "history = cls_model.fit(train_data_sents, train_data_labels,\r\n",
        "                        epochs=NUM_EPOCHS,\r\n",
        "                        batch_size=BATCH_SIZE,\r\n",
        "                        validation_split=VALID_SPLIT,\r\n",
        "                        callbacks=[es_callback, cp_callback])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./data_out\\tf2_gpt2_naver_movie\\weights.h5 directory already exist\n",
            "\n",
            "Epoch 1/3\n",
            "4219/4219 [==============================] - ETA: 0s - loss: 0.3437 - accuracy: 0.8478WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ]
        }
      ],
      "metadata": {
        "id": "yRF8D388xGb5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'], '')\r\n",
        "plt.xlabel('Epochs')\r\n",
        "plt.ylabel('Loss')\r\n",
        "plt.legend(['Loss', 'Validation Loss'])\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "UGj4h0l3xGb9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'], '')\r\n",
        "plt.xlabel('Epochs')\r\n",
        "plt.ylabel('Loss')\r\n",
        "plt.legend(['Accuracy', 'Validation Accuracy'])\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "7x-FC6BDxGcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 평가"
      ],
      "metadata": {
        "id": "YKJx63kSxGcF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "test_data_sents = []\r\n",
        "test_data_labels = []\r\n",
        "\r\n",
        "for test_sent, test_label in test_data[['document','label']].values:\r\n",
        "    test_tokenized_text = vocab[tokenizer(clean_text(test_sent))]\r\n",
        "    \r\n",
        "    tokens = [vocab[vocab.bos_token]]\r\n",
        "    tokens += pad_sequences([test_tokenized_text],\r\n",
        "                            SENT_MAX_LEN,\r\n",
        "                            value=vocab[vocab.padding_token],\r\n",
        "                            padding='post').tolist()[0]\r\n",
        "    tokens += [vocab[vocab.eos_token]]\r\n",
        "    \r\n",
        "    test_data_sents.append(tokens)\r\n",
        "    test_data_labels.append(test_label)\r\n",
        "    \r\n",
        "test_data_sents = np.array(test_data_sents, dtype=np.int64)\r\n",
        "test_data_labels = np.array(test_data_labels, dtype=np.int64)"
      ],
      "outputs": [],
      "metadata": {
        "id": "VywcseLrxGcH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "cls_model.load_weights(checkpoint_path)\r\n",
        "cls_model.evaluate(test_data_sents, test_data_labels, batch_size=1024)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Wj3dRljzxGcP"
      }
    }
  ]
}